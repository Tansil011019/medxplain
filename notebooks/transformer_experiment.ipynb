{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b66cf71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import __main__\n",
    "print(__main__.__package__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffcb54f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cb490e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing for multihead attention\n",
    "from transformer.config.multihead_attention import MultiHeadAttentionConfig\n",
    "\n",
    "multi_attn_conf = MultiHeadAttentionConfig(\n",
    "    embed_dim=2, # size of vector for each token\n",
    "    num_head=1,\n",
    "    dropout=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eacbd4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# check with 1 batch, 1 token, 2 embed dim\n",
    "query = torch.randn(1, 1, 2)\n",
    "key = torch.randn(1, 1, 2)\n",
    "value = torch.randn(1, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5808522a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: tensor([[[0.3367, 0.1288]]])\n",
      "Key: tensor([[[0.2345, 0.2303]]])\n",
      "Value: tensor([[[-1.1229, -0.1863]]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Query: {query}\")\n",
    "print(f\"Key: {key}\")\n",
    "print(f\"Value: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "257778ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer.base.multihead_attention import MultiHeadAttention\n",
    "\n",
    "multi_attn = MultiHeadAttention(multi_attn_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "273fbbde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7658, -0.7506,  1.3525],\n",
       "        [ 0.6863, -0.3278,  0.7950]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "767dfb07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7658,  0.6863],\n",
       "        [-0.7506, -0.3278],\n",
       "        [ 1.3525,  0.7950]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.transpose(x, -2, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b24d4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor([\n",
    "    [\n",
    "        [1.87, 1.09],\n",
    "        [-1.68, 0.67]\n",
    "    ]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71f8b437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Linear Projection\n",
      "Query: tensor([[[ 1.6264,  0.6333],\n",
      "         [-0.2684, -0.5352]]], grad_fn=<ViewBackward0>)\n",
      "Query Shape: torch.Size([1, 2, 2])\n",
      "Key: tensor([[[-0.7080, -0.2305],\n",
      "         [ 0.3881,  0.9608]]], grad_fn=<ViewBackward0>)\n",
      "Key Shape: torch.Size([1, 2, 2])\n",
      "Value: tensor([[[-1.3324, -1.5352],\n",
      "         [ 0.7861, -0.6478]]], grad_fn=<ViewBackward0>)\n",
      "Value Shape: torch.Size([1, 2, 2])\n",
      "==================================================\n",
      "Split Num Head\n",
      "Query: tensor([[[[ 1.6264,  0.6333],\n",
      "          [-0.2684, -0.5352]]]], grad_fn=<TransposeBackward0>)\n",
      "Query Shape: torch.Size([1, 1, 2, 2])\n",
      "Key: tensor([[[[-0.7080, -0.2305],\n",
      "          [ 0.3881,  0.9608]]]], grad_fn=<TransposeBackward0>)\n",
      "Key Shape: torch.Size([1, 1, 2, 2])\n",
      "Value: tensor([[[[-1.3324, -1.5352],\n",
      "          [ 0.7861, -0.6478]]]], grad_fn=<TransposeBackward0>)\n",
      "Value Shape: torch.Size([1, 1, 2, 2])\n",
      "==================================================\n",
      "Scaled Dot Products\n",
      "K transpose: tensor([[[[-0.7080,  0.3881],\n",
      "          [-0.2305,  0.9608]]]], grad_fn=<TransposeBackward0>)\n",
      "Head dimension: 2\n",
      "Attention score: tensor([[[[-0.9175,  0.8766],\n",
      "          [ 0.2216, -0.4373]]]], grad_fn=<DivBackward0>)\n",
      "Attention Probability: tensor([[[[0.1426, 0.8574],\n",
      "          [0.6590, 0.3410]]]], grad_fn=<SoftmaxBackward0>)\n",
      "Attention Probability After Dropout: tensor([[[[0.2037, 1.2249],\n",
      "          [0.0000, 0.4871]]]], grad_fn=<MulBackward0>)\n",
      "Output: tensor([[[[ 0.6915, -1.1062],\n",
      "          [ 0.3829, -0.3156]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "==================================================\n",
      "Combining Head\n",
      "Attention Output: tensor([[[ 0.6915, -1.1062],\n",
      "         [ 0.3829, -0.3156]]], grad_fn=<ViewBackward0>)\n",
      "Attention Output Shape: torch.Size([1, 2, 2])\n",
      "==================================================\n",
      "Output: tensor([[[0.8764, 0.6843],\n",
      "         [0.2045, 0.6089]]], grad_fn=<ViewBackward0>)\n",
      "Output Shape: torch.Size([1, 2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.8764, 0.6843],\n",
       "         [0.2045, 0.6089]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_attn(x, x, x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
